# 1. Problem Framing
We are using the Walmart dataset explored in the earlier project, primarily for continuity. With a good level of understanding of its adversarial elements, we believe this project will help us explore whether tools beyond classical time-series models can meaningfully engage with clustered volatility. We have a document exclusively created to address the “Why” questions which will give more information on Baseline Diagnosis

# 2. Regression with Lag Features
When we attempted regression with lag features, we found that the monthly aggregation used earlier for ARIMA was insufficient for evaluating regression and ML behavior. To enable a more meaningful comparison between regression and ML models, we expanded the dataset to weekly granularity, yielding 208 data points (52 weeks × 4 years). Using daily data was avoided, as it would have amplified high-frequency noise that was already observed in earlier projects. Both dense lag structures (13 continuous weeks) and sparse lag structures (weeks 1, 4, 12, and 52) were attempted. For evaluation we used  simple chronological 80:20 train–test split.
Plain regression with lag features performed poorly. The model failed to resonate with the volatile highs and lows and instead attempted to follow directional changes with noticeable delay, often lagging by multiple weeks. Dense lag configurations produced more fluctuation, largely reflecting noise, while sparse lag configurations smoothed the series and responded only at the specified lag intervals. In both cases, the regression model was unable to sustain extreme movements and collapsed quickly toward a smoothed mean, indicating that noise dominates any lagged linear relationship.
At this stage, no obvious leakage was observed, since the train–test split was performed chronologically. However, this does not eliminate leakage risk entirely; it only indicates that no explicit leakage was present under the current setup.

# 3. ML with Lag Features
We used XGBoost with lag features only, and evaluated it under two setups:
## (a) one-step-ahead forecasting with an expanding window, and
## (b) multi-step roll out forecasting using a 60:20:20 chronological split.
In the one-step setup, performance was clearly better than linear regression. The model reflected volatility more closely and even over-reacted to a pronounced peak near the end of the first year. However, as training progressed, predictions became increasingly regulated. Repeated spikes and drops were dampened, and the final peak in the original series near the end of the training period was captured only weakly and with a noticeable lag of three to four months.
In the multi-step roll out setting, training-period predictions appeared substantially accurate, which is expected since the model was learning directly from historical values. Even within training, however, the model consistently failed to reach the full magnitude of peaks and troughs. Performance degraded further in the validation window, where predictions showed delayed directional movement and reduced responsiveness compared to the training phase. In the test window, the model failed to capture even short-term behavior, reflecting the absence of stable patterns in the underlying series.
Overall, the one-step configuration initially mirrored volatility convincingly but gradually converged toward smoother behavior. In contrast, the multi-step roll out exhibited strong apparent performance in training but deteriorated rapidly in validation and testing, with no sustained predictive capability.

# 4. Illusions of Success
The first experiment involved intentional feature leakage. An ordinal feature was introduced that encoded the sales value of the same week into five buckets — Trough, Low, Moderate, High, and Peak — relative to the global average weekly sales. This feature implicitly carried future information about the target magnitude.
Visually, both the expanding-window setup and the 60:20:20 split showed noticeably improved alignment, suggesting that contextual information about relative sales magnitude helps the model track the series better. However, a closer look at the metrics revealed that the expanding-window forecasts did not actually improve, with MSE and RMSE remaining close to earlier values. We realized that under an expanding window, each prediction was trained on almost the same history, differing by only one additional week. This produced visually smooth plots with apparently correct slopes, but without corresponding value accuracy. The visual improvement reflected directional smoothness, not true predictive gain.
In contrast, the 60:20:20 split did not show this discrepancy between visual and numeric performance. This is because the model was trained on a fixed distribution and then applied regime labels to a continuous future segment, effectively assigning magnitude categories to test weeks whose relative scale was already encoded.
The second experiment simulated a common implementation mistake: using recursive predictions in place of past actual values during forecasting. While this does not constitute feature leakage, it represents a frequent coding error that can severely distort evaluation. In this scenario, the expanding-window plot showed dramatic apparent improvement, creating a strong illusion of success. However, the 60:20:20 split did not show meaningful improvement, likely because the test window was relatively small.
To probe further, we reduced lag density and expanded the test window by switching to a 30:20:50 split. Even then, performance did not improve. With sparse lags, the model lost access to recent micro-patterns and local volatility that dense lags had previously provided, leaving it with even weaker signals. The recursive-forecasting setup thus demonstrated how replacing real data uncertainty with model-generated certainty can mask structural weakness. Recursive forecasting, while orthogonal to data leakage, can still create an illusion of success by feeding the model’s own predictions back as inputs.
## 4a. Illusions of Success – Failed attempts
We also explored two additional scenarios. First, we attempted partial future leakage by introducing information from the first three days of the following week into the current week. This did not create an illusion of success, likely because volatility within weekly aggregates is already high, and partial hints about the next week did not meaningfully stabilize predictions.
Second, we experimented with normalization of sales and feature values. This also failed to create an illusion, which led to the realization that XGBoost, being tree-based, is largely insensitive to feature scaling.
Although these additional experiments did not produce misleading success, they were instructive. Together, these exercises reinforced how fragile model evaluation can be and how we can always learn some thing even with our failed efforts.

#5. Exogenous Features Exploration
We first explored calendar-based statistical signals. Monthly directional deviation was computed using median values relative to the global median. Only a couple of months showed consistent directional deviation across all four years, while a few others showed partial consistency limited to specific weeks. This signal was weak and unstable, and insufficient to justify calendar conditioning at the weekly level. This analysis indicated that calendar effects, if present, are not persistent enough to serve as reliable exogenous drivers.
Next, we examined weekly volatility patterns by comparing weekly standard deviation against yearly standard deviation, viewed at a monthly resolution. The results showed a mixed and inconsistent volatility profile across years for the same weeks. This further reduced confidence that calendar-based variance patterns could meaningfully explain demand fluctuations.
With calendar features offering limited value, we moved to event-based reasoning. Using MAD (Median Absolute Deviation), we identified twelve outliers (ten peaks and two troughs), of which only one persisted for two consecutive weeks. The remaining outliers were isolated to single weeks. This suggests that while extreme events do occur, they are sporadic and lack persistence. As a result, event-based exogenous features do not provide a stable or systematic explanation for volatility. To summarize Walmart weekly demand contains a substantial irreducible component that cannot be conditioned on calendar or simple event abstractions.
## 5a. Oracle Plot
For academic completeness, we conducted an oracle-style experiment by injecting these outlier indicators along with dense lag features in a one-step-ahead forecasting setup. Although the resulting predictions appeared visually impressive after injecting information about the identified extreme cases, the model failed to generalize to subsequent spikes and troughs beyond that curated set. Accurate performance required continual intervention, indicating poor generalization. This final experiment reinforced the conclusion that even with calendar and event-based exogenous features, the model does not achieve sustainable predictive capability in a forward-looking setting.

# 6. Comparative Synthesis
When we applied ARIMA to the Walmart dataset, two observations stood out. First, there was no meaningful information carried forward beyond the immediate step, and short-horizon behavior collapsed toward a mean-only description of the series. Second, while the residuals were centered around zero—indicating that the mean had been modeled—their magnitudes appeared in clusters, revealing time-dependent volatility.
ML models could have added value if non-linear relationships had emerged through longer lag structures or if meaningful calendar effects or exogenous drivers had been identifiable. However, systematic exploration using both dense and sparse lags, as well as calendar- and event-based reasoning, failed to uncover such signals. Predictive performance did not improve in a sustained or generalizable way.
Only during academic experiments designed to expose illusions of success did the ML model appear to perform well—specifically under intentional data leakage or under common implementation mistakes such as recursive forecasting using model outputs instead of actual. These scenarios produced visually convincing results without genuine learning.
Beyond these artificial conditions, ML did not uncover stable, generalizable predictive structure; however, it did provide additional insight by exhaustively probing non-linear dependencies, longer lags, and potential exogenous signals. This broader exploration confirmed that the absence of structure in the Walmart dataset is deeper and more fundamental than what ARIMA alone could establish.

# 7. Key Takeaways
This project taught us that that the volatility can exist in various degrees. While ML model may be able to handle volatility if it contains few elements that can be put together for the short term memory and to represent calender effect. When lack of structure becomes the definition of dataset, even models with larger capacity like ML models cannot address it.
General models (like ML with controlled capacity) can expand what is possible after a point where simple models stop. On the other hand, Simple models (Like ARIMA and Holt) can just give a hint about what is unknowable for them
There were lot of lessons learnt during the implementation of the project which have  been captured separately under various heads which will be of immense use for future projects.

# 8. Artifacts
Jupyter Notebooks
    • Walmart_Arima.ipynb
    • Walmart_Linear Model.ipynb
    • Walmart_ML Model.ipynb
    • Walmart_ML_Illusions.ipynb
    • Walmart_Calnder and Exhogenous features.ipynb
Supporting Documents (optional links)
    • README_First.md
    • README.md
    • Lessons Learnt.md
    • Intutitive Learnings.md
